{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a85dcb8a-8425-4ecd-96cb-0b986f97180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, OneHotEncoder, StringIndexer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac771f-701d-4282-999d-7abc2aba428a",
   "metadata": {},
   "source": [
    "### Read the pickup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54c23c-85e3-4f04-9fa3-9916c15690aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_jdbc_jar = r\"C:/Program Files/PostgreSQL/17/postgresql-42.7.4.jar\"\n",
    "spark = SparkSession.builder.appName('FeatureEngineering')\\\n",
    "                            .config(\"spark.jars\", postgresql_jdbc_jar) \\\n",
    "                            .config(\"spark.driver.extraClassPath\", postgresql_jdbc_jar) \\\n",
    "                            .config(\"spark.driver.memory\", \"8g\")\\\n",
    "                            .config(\"spark.executor.memory\", \"8g\")\\\n",
    "                            .config(\"spark.executor.cores\", \"4\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1ec3d7b-3cf1-4708-8289-bb456cdeb1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 4005691\n"
     ]
    }
   ],
   "source": [
    "# Database connection parameters\n",
    "url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Query to filter cities Hangzhou and Shanghai\n",
    "query = \"(SELECT * FROM pickup_data WHERE city IN ('Chongqing', 'Shanghai', 'Yantai', 'Jilin')) AS filtered_data\"\n",
    "\n",
    "# Load the data into a PySpark DataFrame\n",
    "df_pickup = spark.read.jdbc(url=url, table=query, properties=properties)\n",
    "\n",
    "# Show the first few rows\n",
    "row_count = df_pickup.count()\n",
    "print(f\"Number of rows in the DataFrame: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e53afad0-f4e3-4174-92bd-9b2fe2165cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- region_id: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- courier_id: integer (nullable = true)\n",
      " |-- accept_time: timestamp (nullable = true)\n",
      " |-- time_window_start: timestamp (nullable = true)\n",
      " |-- time_window_end: timestamp (nullable = true)\n",
      " |-- aoi_id: integer (nullable = true)\n",
      " |-- aoi_type: integer (nullable = true)\n",
      " |-- pickup_time: timestamp (nullable = true)\n",
      " |-- ds: integer (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- pickup_gps_lng: double (nullable = true)\n",
      " |-- pickup_gps_lat: double (nullable = true)\n",
      " |-- accept_gps_lng: double (nullable = true)\n",
      " |-- accept_gps_lat: double (nullable = true)\n",
      " |-- pickup_eta_minutes: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pickup.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69f36d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickup = df_pickup.drop('pickup_eta_minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a76a7a-8d0b-4ebc-96f8-551ffcd9496b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a968e69-9311-4454-9c2b-7df57f140da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|pickup_eta_minutes|pickup_time_delay|\n",
      "+------------------+-----------------+\n",
      "|              95.0|             91.0|\n",
      "|              95.0|             50.0|\n",
      "|              95.0|            185.0|\n",
      "|              95.0|             38.0|\n",
      "|              95.0|             84.0|\n",
      "|               0.0|             48.0|\n",
      "|              96.0|             33.0|\n",
      "|              96.0|             33.0|\n",
      "|              96.0|             10.0|\n",
      "|               0.0|             58.0|\n",
      "+------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, mean\n",
    "\n",
    "# Extracting hour, day of the week, and month from the `accept_time` column\n",
    "df_pickup = df_pickup.withColumn(\"hour_of_day\", F.hour(df_pickup[\"accept_time\"])) \\\n",
    "       .withColumn(\"day_of_week\", F.dayofweek(df_pickup[\"accept_time\"])) \\\n",
    "       .withColumn(\"month\", F.month(df_pickup[\"accept_time\"]))\n",
    "\n",
    "# Calculate time taken for pickup (in minutes) which is already calculated in pickup_eta_minutes\n",
    "\n",
    "# Calculate the time difference between expected and actual pickup time in minutes\n",
    "# expected_pickup_time = time_window_start\n",
    "# actual pickup time = pickup_time\n",
    "# A positive value means the pickup happened after the expected time (late pickup), \n",
    "# while a negative value means it happened earlier than expected (early pickup).\n",
    "\n",
    "# # Compute avg_pickup_time_minutes\n",
    "# avg_pickup_time = df_pickup.select(expr(\"percentile_approx(pickup_eta_minutes, 0.50)\")).collect()[0][0]\n",
    "\n",
    "# # Compute pickup_time_delay\n",
    "# df_pickup = df_pickup.withColumn(\"pickup_time_delay\", col(\"pickup_eta_minutes\") - avg_pickup_time)\n",
    "\n",
    "df_pickup = df_pickup.withColumn(\"pickup_eta_minutes\",\n",
    "                  (df_pickup[\"time_window_start\"].cast(\"long\") - df_pickup[\"accept_time\"].cast(\"long\")) / 60)\n",
    "\n",
    "df_pickup = df_pickup.withColumn(\"pickup_time_delay\",\n",
    "                  (df_pickup[\"pickup_time\"].cast(\"long\") - df_pickup[\"accept_time\"].cast(\"long\")) / 60)\n",
    "\n",
    "# Show result\n",
    "df_pickup.select(\"pickup_eta_minutes\", \"pickup_time_delay\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b4d715de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative values in column 'pickup_time_delay': 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Replace 'column_name' with your actual column name\n",
    "negative_count = df_pickup.filter(col(\"pickup_eta_minutes\") < 0).count()\n",
    "\n",
    "print(f\"Number of negative values in column 'pickup_time_delay': {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8c5bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickup = df_pickup.filter(col(\"pickup_eta_minutes\") >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1ec02c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|pickup_time_delay|\n",
      "+-------+-----------------+\n",
      "|  count|          3976707|\n",
      "|   mean|216.8401511602439|\n",
      "| stddev|412.8116907220085|\n",
      "|    min|              0.0|\n",
      "|    25%|             63.0|\n",
      "|    50%|            116.0|\n",
      "|    75%|            192.0|\n",
      "|    max|          54859.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pickup.select(\"pickup_time_delay\").summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c74b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # looking above pickup_time_delay distribution is still highly skewed, with a mean of ~101 minutes but a median (50%) of 0. \n",
    "# # The max value of 54,744 minutes (~38 days) suggests there are extreme outliers.\n",
    "# # Handle Outliers (Max Value = 54,744), Use the 99th percentile to filter extreme delays\n",
    "# threshold = df_pickup.approxQuantile(\"pickup_time_delay\", [0.99], 0.01)[0]\n",
    "# df_pickup = df_pickup.filter(col(\"pickup_time_delay\") <= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pickup = df_pickup.withColumn(\n",
    "#     \"pickup_time_delay\", \n",
    "#     F.when(col(\"pickup_time_delay\") < -30, 0).otherwise(col(\"pickup_time_delay\"))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b6a010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 3976707\n"
     ]
    }
   ],
   "source": [
    "total_rows = df_pickup.count()\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab4f9ce3-8a60-4769-abe0-9deff2ca7b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+-----------------+\n",
      "|hour_of_day|day_of_week|month|pickup_time_delay|\n",
      "+-----------+-----------+-----+-----------------+\n",
      "|          7|          7|    6|            529.0|\n",
      "|          7|          7|    6|             32.0|\n",
      "|          7|          7|    6|            513.0|\n",
      "|          7|          7|    6|            115.0|\n",
      "|          7|          7|    6|            277.0|\n",
      "+-----------+-----------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pickup.select(\"hour_of_day\", \"day_of_week\", \"month\", \"pickup_time_delay\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0852da0c-fb31-498d-8697-b075d8c9b614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------------+\n",
      "|order_id|city|pickup_time_delay|\n",
      "+--------+----+-----------------+\n",
      "+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where pickup_time_difference is negative\n",
    "df_negative_pickups = df_pickup.filter(df_pickup[\"pickup_time_delay\"] < 0)\n",
    "df_negative_pickups.select(\"order_id\",\"city\", \"pickup_time_delay\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d56ef-ba2b-4bf8-936a-ba232c282b70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Geospatial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc70ee-ba7c-4636-9ea7-0708d64225c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Calculate the distance between pickup and delivery locations using Haversine Formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "607794e8-30b6-48b0-9e9d-9fc5764e425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Haversine function to calculate distance between two points on the Earth\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    \n",
    "    dlat = radians(lat2 - lat1)  # Difference in latitudes (radians)\n",
    "    dlon = radians(lon2 - lon1)  # Difference in longitudes (radians)\n",
    "    \n",
    "    # Apply Haversine formula to calculate the \"a\" value\n",
    "    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2\n",
    "    # calculate the central angle between two points\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    # Compute the distance in KM using the radius of Earth (R) and the central angle (c)\n",
    "    distance = R * c \n",
    "    return distance\n",
    "\n",
    "# Apply the Haversine function to the DataFrame to calculate the distance in KM\n",
    "df_pickup = df_pickup.withColumn(\"pickup_distance_km\", \n",
    "                                 haversine(df_pickup[\"pickup_gps_lat\"], df_pickup[\"pickup_gps_lng\"],\n",
    "                                           df_pickup[\"accept_gps_lat\"], df_pickup[\"accept_gps_lng\"]))\n",
    "# Round the 'pickup_distance_km' column to 2 decimal places\n",
    "df_pickup = df_pickup.withColumn(\"pickup_distance_km\", round(df_pickup[\"pickup_distance_km\"], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ace008e6-0625-470b-ae64-fe13911ce47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------+--------------+--------------+------------------+\n",
      "|order_id|pickup_gps_lat|pickup_gps_lng|accept_gps_lat|accept_gps_lng|pickup_distance_km|\n",
      "+--------+--------------+--------------+--------------+--------------+------------------+\n",
      "| 3764982|      32.57753|     118.38133|      32.70376|     118.54136|             20.53|\n",
      "| 5767762|      37.56164|     121.27105|      37.54782|     121.25854|              1.89|\n",
      "| 3477155|      31.01129|     121.21426|      31.00848|     121.19963|              1.43|\n",
      "| 2909859|       31.3071|      121.4905|      31.30869|     121.49028|              0.18|\n",
      "| 3520728|      31.23623|     121.72127|      31.23602|     121.71976|              0.15|\n",
      "|  940606|      32.57753|     118.38133|      32.70376|     118.54136|             20.53|\n",
      "| 2373275|      29.70774|     107.37643|      29.70717|     107.38724|              1.05|\n",
      "| 1022562|       31.3081|     121.49443|       31.3077|     121.49027|               0.4|\n",
      "| 5449301|      29.38246|     106.51069|      29.38249|     106.51391|              0.31|\n",
      "| 1676134|      32.57753|     118.38133|      29.59351|     106.23172|           1202.79|\n",
      "+--------+--------------+--------------+--------------+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pickup.select(\"order_id\",\"pickup_gps_lat\",\"pickup_gps_lng\",\"accept_gps_lat\",\"accept_gps_lng\",\"pickup_distance_km\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a8c01-c69b-415e-bae5-9f20679d8174",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Cluster regions using K-Means or DBSCAN for better route mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0132ce11-71d6-4efd-b5a0-9a18a92948ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observing above regions and number of orders they have we can see here is a large variation in the number of rows (packages) across regions, \n",
    "# DBSCAN would be a more appropriate clustering algorithm for this scenario as It can handle regions where some areas are densely packed with \n",
    "# packages (e.g., urban centers) while others have fewer packages (e.g., rural areas).\n",
    "# Also it doesn't require the number of clusters to be predefined and can naturally identify clusters of different shapes and sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e388d01f-fa2a-4d9a-9f2b-f4bb2d8764bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select the relevant geographic columns (Longitude and Latitude)\n",
    "feature_columns = [\"pickup_gps_lng\", \"pickup_gps_lat\"]\n",
    "\n",
    "# Step 2: Vectorize the geographic features into a single vector column 'features'\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_vectorized = vector_assembler.transform(df_pickup)\n",
    "\n",
    "# Step 4: Apply KMeans with a specified number of clusters (let's say 5)\n",
    "kmeans = KMeans(k=25, seed=1, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(df_vectorized)\n",
    "\n",
    "# Step 5: Make predictions (assign clusters to the data)\n",
    "predictions = model.transform(df_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eaa2a971-743d-47fa-b4c9-58480a03d313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|order_id|     city|cluster|\n",
      "+--------+---------+-------+\n",
      "| 3764982| Shanghai|      2|\n",
      "| 5767762|   Yantai|      1|\n",
      "| 3477155| Shanghai|     19|\n",
      "| 2909859| Shanghai|     20|\n",
      "| 3520728| Shanghai|      4|\n",
      "|  940606|Chongqing|      2|\n",
      "| 2373275|Chongqing|     14|\n",
      "| 1022562| Shanghai|     20|\n",
      "| 5449301|Chongqing|     23|\n",
      "| 1676134|Chongqing|      2|\n",
      "| 1973550|Chongqing|      0|\n",
      "| 5278857|    Jilin|     21|\n",
      "| 2853126|   Yantai|     16|\n",
      "| 5664753| Shanghai|     20|\n",
      "|  207390|    Jilin|      2|\n",
      "| 6118921| Shanghai|      4|\n",
      "| 3068924|Chongqing|     14|\n",
      "|  723232|Chongqing|      0|\n",
      "| 1345159|Chongqing|     23|\n",
      "| 4397326|Chongqing|      2|\n",
      "+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Show some of the prediction results (assigned cluster labels)\n",
    "predictions.select(\"order_id\", \"city\", \"cluster\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f006f631-689c-48fc-81e6-60b3e1b8c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.drop('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d4e72-fd8e-4809-883e-28b7de29900f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Courier & Package Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6729b-8dc0-458a-9ee6-d7195d89b4f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Compute the average pickup time per courier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3966950-7c05-4976-9272-a09537c9892c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+-------------------+-------------------+-------------------+------+--------+-------------------+---+---------+--------+--------------+--------------+--------------+--------------+-----------+-----------+-----+------------------+-----------------+------------------+-------+\n",
      "|order_id|region_id|    city|courier_id|        accept_time|  time_window_start|    time_window_end|aoi_id|aoi_type|        pickup_time| ds|      lng|     lat|pickup_gps_lng|pickup_gps_lat|accept_gps_lng|accept_gps_lat|hour_of_day|day_of_week|month|pickup_eta_minutes|pickup_time_delay|pickup_distance_km|cluster|\n",
      "+--------+---------+--------+----------+-------------------+-------------------+-------------------+------+--------+-------------------+---+---------+--------+--------------+--------------+--------------+--------------+-----------+-----------+-----+------------------+-----------------+------------------+-------+\n",
      "| 3764982|       21|Shanghai|      5073|2025-08-13 15:25:00|2025-08-13 17:00:00|2025-08-13 19:00:00| 21908|      14|2025-08-13 16:56:00|813|121.34478|31.41183|     118.38133|      32.57753|     118.54136|      32.70376|         15|          4|    8|              95.0|             91.0|             20.53|      2|\n",
      "| 5767762|      111|  Yantai|      6972|2025-08-13 15:25:00|2025-08-13 17:00:00|2025-08-13 19:00:00|   596|      14|2025-08-13 16:15:00|813|121.27093|37.56191|     121.27105|      37.56164|     121.25854|      37.54782|         15|          4|    8|              95.0|             50.0|              1.89|      1|\n",
      "| 3477155|       12|Shanghai|     14790|2025-08-13 15:25:00|2025-08-13 17:00:00|2025-08-13 19:00:00| 22345|       1|2025-08-13 18:30:00|813|121.21017|31.01309|     121.21426|      31.01129|     121.19963|      31.00848|         15|          4|    8|              95.0|            185.0|              1.43|     19|\n",
      "| 2909859|       20|Shanghai|     11311|2025-08-13 15:25:00|2025-08-13 17:00:00|2025-08-13 19:00:00| 23118|       1|2025-08-13 16:03:00|813|121.49122|31.30721|      121.4905|       31.3071|     121.49028|      31.30869|         15|          4|    8|              95.0|             38.0|              0.18|     20|\n",
      "| 3520728|       39|Shanghai|      9580|2025-08-13 15:25:00|2025-08-13 17:00:00|2025-08-13 19:00:00|  5551|      14|2025-08-13 16:49:00|813|121.71922|31.23868|     121.72127|      31.23623|     121.71976|      31.23602|         15|          4|    8|              95.0|             84.0|              0.15|      4|\n",
      "+--------+---------+--------+----------+-------------------+-------------------+-------------------+------+--------+-------------------+---+---------+--------+--------------+--------------+--------------+--------------+-----------+-----------+-----+------------------+-----------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by courier_id and calculate the average pickup time delay (in minutes)\n",
    "average_pickup_time_per_courier = predictions.groupBy(\"courier_id\").agg(\n",
    "    F.avg(\"pickup_time_delay\").alias(\"avg_pickup_time_minutes\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "predictions.show(5)\n",
    "predictions = predictions.join(\n",
    "    average_pickup_time_per_courier, on=\"courier_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bbaad-a327-42bc-8baf-d7bc7b3ae216",
   "metadata": {},
   "source": [
    "#### Extract package pickup patterns (e.g., peak hours, busiest locations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60152b9b-db29-4ed8-b944-e9e2f3910ce2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Peak hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34713424-c095-4534-a19b-30622a39de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------------+\n",
      "|order_id|hour_of_day|pickup_order_count|\n",
      "+--------+-----------+------------------+\n",
      "| 3764982|         15|            170936|\n",
      "| 5767762|         15|            170936|\n",
      "| 3477155|         15|            170936|\n",
      "| 2909859|         15|            170936|\n",
      "| 3520728|         15|            170936|\n",
      "+--------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Group by hour_of_day and count the number of pickups (orders)\n",
    "pickup_by_hour = predictions.groupBy(\"hour_of_day\").agg(\n",
    "    F.count(\"order_id\").alias(\"pickup_order_count\")\n",
    ")\n",
    "\n",
    "# Step 2: Sort by the pickup count in descending order to find the peak hours\n",
    "pickup_by_hour_sorted = pickup_by_hour.orderBy(F.col(\"pickup_order_count\").desc())\n",
    "\n",
    "# Step 3: Join the pickup count by hour to the predictions DataFrame based on hour_of_day\n",
    "predictions = predictions.join(\n",
    "    pickup_by_hour_sorted, on=\"hour_of_day\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Show the result: predictions with pickup count by hour\n",
    "predictions.select(\"order_id\", \"hour_of_day\", \"pickup_order_count\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c9eba-b762-4719-861d-0652e45cb106",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Busiest locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b371d107-5745-4b58-83d2-fa42ac887a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|     city|city_order_count|\n",
      "+---------+----------------+\n",
      "| Shanghai|         1409554|\n",
      "|Chongqing|         1164146|\n",
      "|   Yantai|         1142467|\n",
      "|    Jilin|          260540|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_busiest_locations = predictions.groupBy(\"city\") \\\n",
    "    .agg(F.count(\"order_id\").alias(\"city_order_count\")) \\\n",
    "    .orderBy(F.desc(\"city_order_count\"))\n",
    "df_busiest_locations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a99116b4-4f21-4245-b48d-ae2a09890dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Join the pickup count by hour to the predictions DataFrame based on hour_of_day\n",
    "predictions = predictions.join(\n",
    "    df_busiest_locations, on=\"city\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "21f2c79a-7b84-4d0c-b066-0c4b8ceaedbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------------+\n",
      "|order_id|     city|city_order_count|\n",
      "+--------+---------+----------------+\n",
      "| 5940108|Chongqing|         1164146|\n",
      "| 1378407|Chongqing|         1164146|\n",
      "| 4725304|Chongqing|         1164146|\n",
      "| 6142927|Chongqing|         1164146|\n",
      "|  821810|   Yantai|         1142467|\n",
      "+--------+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"order_id\", \"city\", \"city_order_count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88f27dce-200d-4357-91cd-0bb9502af04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- courier_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- region_id: integer (nullable = true)\n",
      " |-- accept_time: timestamp (nullable = true)\n",
      " |-- time_window_start: timestamp (nullable = true)\n",
      " |-- time_window_end: timestamp (nullable = true)\n",
      " |-- aoi_id: integer (nullable = true)\n",
      " |-- aoi_type: integer (nullable = true)\n",
      " |-- pickup_time: timestamp (nullable = true)\n",
      " |-- ds: integer (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- pickup_gps_lng: double (nullable = true)\n",
      " |-- pickup_gps_lat: double (nullable = true)\n",
      " |-- accept_gps_lng: double (nullable = true)\n",
      " |-- accept_gps_lat: double (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- pickup_eta_minutes: double (nullable = true)\n",
      " |-- pickup_time_delay: double (nullable = true)\n",
      " |-- pickup_distance_km: double (nullable = true)\n",
      " |-- cluster: integer (nullable = false)\n",
      " |-- avg_pickup_time_minutes: double (nullable = true)\n",
      " |-- pickup_order_count: long (nullable = true)\n",
      " |-- city_order_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a0a5f-0bbd-42e4-888e-9ae88156e72c",
   "metadata": {},
   "source": [
    "### Anomaly Detection Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91425ade-4689-4ee9-8f0f-43eaf1138c7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Identify delays based on time threshold deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "814cf334-7b60-48dd-8bc2-8e93014761d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+-------------------+-------------------+----------+\n",
      "|order_id|     city|pickup_time_delay|        pickup_time|        accept_time|is_delayed|\n",
      "+--------+---------+-----------------+-------------------+-------------------+----------+\n",
      "|  998888|    Jilin|            176.0|2025-10-22 12:08:00|2025-10-22 09:12:00|   On Time|\n",
      "| 1817267|Chongqing|             27.0|2025-10-22 09:39:00|2025-10-22 09:12:00|   On Time|\n",
      "| 5524389|   Yantai|             27.0|2025-10-22 09:39:00|2025-10-22 09:12:00|   On Time|\n",
      "| 3124312|Chongqing|            132.0|2025-10-22 11:24:00|2025-10-22 09:12:00|   On Time|\n",
      "|  221410| Shanghai|            106.0|2025-10-22 10:58:00|2025-10-22 09:12:00|   On Time|\n",
      "| 3287344|Chongqing|            115.0|2025-10-22 11:07:00|2025-10-22 09:12:00|   On Time|\n",
      "|  964270| Shanghai|            389.0|2025-10-22 15:41:00|2025-10-22 09:12:00|   Delayed|\n",
      "| 3835946|   Yantai|            176.0|2025-10-22 12:08:00|2025-10-22 09:12:00|   On Time|\n",
      "| 3408841|   Yantai|           1833.0|2025-10-23 15:45:00|2025-10-22 09:12:00|   Delayed|\n",
      "| 5452642| Shanghai|             99.0|2025-10-22 10:51:00|2025-10-22 09:12:00|   On Time|\n",
      "| 4972100|Chongqing|             17.0|2025-10-22 09:29:00|2025-10-22 09:12:00|   On Time|\n",
      "| 5950591| Shanghai|            247.0|2025-10-22 13:19:00|2025-10-22 09:12:00|   On Time|\n",
      "| 1084012|   Yantai|            460.0|2025-10-22 16:52:00|2025-10-22 09:12:00|   Delayed|\n",
      "| 1669840|Chongqing|            259.0|2025-10-22 13:31:00|2025-10-22 09:12:00|   On Time|\n",
      "| 1649197|Chongqing|             63.0|2025-10-22 10:15:00|2025-10-22 09:12:00|   On Time|\n",
      "| 2324275|   Yantai|            195.0|2025-10-22 12:27:00|2025-10-22 09:12:00|   On Time|\n",
      "| 2805523|Chongqing|             98.0|2025-10-22 10:50:00|2025-10-22 09:12:00|   On Time|\n",
      "|  377878| Shanghai|            249.0|2025-10-22 13:21:00|2025-10-22 09:12:00|   On Time|\n",
      "| 3649709|   Yantai|             65.0|2025-10-22 10:17:00|2025-10-22 09:12:00|   On Time|\n",
      "| 3350698| Shanghai|            148.0|2025-10-22 11:40:00|2025-10-22 09:12:00|   On Time|\n",
      "+--------+---------+-----------------+-------------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the threshold for delay\n",
    "time_threshold = 360  # Set the threshold in minutes\n",
    "\n",
    "# Step 2: Identify delays based on the pickup time delay\n",
    "# We create a new column `is_delayed` that indicates if the delay is above the threshold\n",
    "predictions = predictions.withColumn(\n",
    "    \"is_delayed\", \n",
    "    F.when(F.col(\"pickup_time_delay\") > time_threshold, \"Delayed\").otherwise(\"On Time\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame with the delay identification\n",
    "predictions.select(\n",
    "    \"order_id\", \n",
    "    \"city\", \n",
    "    \"pickup_time_delay\", \n",
    "    \"pickup_time\", \n",
    "    \"accept_time\", \n",
    "    \"is_delayed\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f339327-6698-4fa9-882f-c35fdbf3e298",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Flag inconsistent geospatial entries (e.g., unrealistic speed between locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40a6e3c7-516e-4093-9605-5a1f0173cf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------------+-----------------+---------+-----------------+\n",
      "|order_id|     city|pickup_distance_km|pickup_time_delay|speed_kmh|     speed_status|\n",
      "+--------+---------+------------------+-----------------+---------+-----------------+\n",
      "|  998888|    Jilin|              0.19|            176.0|        0|  Realistic Speed|\n",
      "| 1817267|Chongqing|              0.34|             27.0|        0|  Realistic Speed|\n",
      "| 5524389|   Yantai|            510.99|             27.0|     1135|Unrealistic Speed|\n",
      "| 3124312|Chongqing|             20.53|            132.0|        9|  Realistic Speed|\n",
      "|  221410| Shanghai|              0.64|            106.0|        0|  Realistic Speed|\n",
      "| 3287344|Chongqing|              0.49|            115.0|        0|  Realistic Speed|\n",
      "|  964270| Shanghai|              0.45|            389.0|        0|  Realistic Speed|\n",
      "| 3835946|   Yantai|              0.56|            176.0|        0|  Realistic Speed|\n",
      "| 3408841|   Yantai|            590.97|           1833.0|       19|  Realistic Speed|\n",
      "| 5452642| Shanghai|              0.19|             99.0|        0|  Realistic Speed|\n",
      "| 4972100|Chongqing|              0.19|             17.0|        0|  Realistic Speed|\n",
      "| 5950591| Shanghai|            341.02|            247.0|       82|  Realistic Speed|\n",
      "| 1084012|   Yantai|              0.48|            460.0|        0|  Realistic Speed|\n",
      "| 1669840|Chongqing|           1203.85|            259.0|      278|Unrealistic Speed|\n",
      "| 1649197|Chongqing|              0.31|             63.0|        0|  Realistic Speed|\n",
      "| 2324275|   Yantai|              0.55|            195.0|        0|  Realistic Speed|\n",
      "| 2805523|Chongqing|              0.19|             98.0|        0|  Realistic Speed|\n",
      "|  377878| Shanghai|              0.14|            249.0|        0|  Realistic Speed|\n",
      "| 3649709|   Yantai|             20.53|             65.0|       18|  Realistic Speed|\n",
      "| 3350698| Shanghai|              0.11|            148.0|        0|  Realistic Speed|\n",
      "+--------+---------+------------------+-----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert pickup_time_delay from minutes to hours\n",
    "predictions = predictions.withColumn(\n",
    "    \"time_diff_hours\", \n",
    "    F.col(\"pickup_time_delay\") / 60  # Convert minutes to hours\n",
    ")\n",
    "\n",
    "# Step 2: Calculate the speed in km/h using the existing pickup_distance_km column\n",
    "predictions = predictions.withColumn(\n",
    "    \"speed_kmh\", \n",
    "    F.when(F.col(\"time_diff_hours\") > 0, F.floor(F.col(\"pickup_distance_km\") / F.col(\"time_diff_hours\"))).otherwise(0)\n",
    ")\n",
    "\n",
    "# Step 3: Flag entries with unrealistic speed\n",
    "speed_threshold = 100  # Define a reasonable speed threshold (in km/h)\n",
    "predictions = predictions.withColumn(\n",
    "    \"speed_status\", \n",
    "    F.when(F.col(\"speed_kmh\") > speed_threshold, \"Unrealistic Speed\").otherwise(\"Realistic Speed\")\n",
    ")\n",
    "\n",
    "predictions = predictions.drop(\"time_diff_hours\")\n",
    "\n",
    "# Show the resulting DataFrame with the speed status column\n",
    "predictions.select(\n",
    "    \"order_id\", \n",
    "    \"city\", \n",
    "    \"pickup_distance_km\", \n",
    "    \"pickup_time_delay\",  # Using existing column for time delay in minutes\n",
    "    \"speed_kmh\", \n",
    "    \"speed_status\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c568c42-16a0-466f-89d4-86033c3b6546",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e682c45f-5613-4894-a5af-984541bbba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.drop('features','scaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0b13084-9bf4-4856-acc7-27e81bc9420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf\n",
    "\n",
    "# # Convert vector column to array of doubles (e.g., 'features' or 'scaled_features')\n",
    "# def vector_to_array(vec):\n",
    "#     if isinstance(vec, Vector):\n",
    "#         return vec.toArray().tolist()\n",
    "#     else:\n",
    "#         return []\n",
    "\n",
    "# # Register the UDF\n",
    "# vector_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "\n",
    "# # Apply the UDF to your vector columns and convert them into array of doubles\n",
    "# predictions = predictions.withColumn(\"pca_features_array\", vector_to_array_udf(\"pca_features\"))\n",
    "\n",
    "# # Drop the original vector columns (if you no longer need them)\n",
    "# predictions = predictions.drop(\"pca_features\")  # Keep 'pca_features_array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5172a9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'hour_of_day', 'courier_id', 'order_id', 'region_id', 'accept_time', 'time_window_start', 'time_window_end', 'aoi_id', 'aoi_type', 'pickup_time', 'ds', 'lng', 'lat', 'pickup_gps_lng', 'pickup_gps_lat', 'accept_gps_lng', 'accept_gps_lat', 'day_of_week', 'month', 'pickup_eta_minutes', 'pickup_time_delay', 'pickup_distance_km', 'cluster', 'avg_pickup_time_minutes', 'pickup_order_count', 'city_order_count', 'is_delayed', 'speed_kmh', 'speed_status']\n"
     ]
    }
   ],
   "source": [
    "print(predictions.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a261bd7-19f3-49a9-95f2-80e1b6ed9dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved to Postgresql successfully\n"
     ]
    }
   ],
   "source": [
    "selected_columns = [\n",
    "    \"order_id\",\n",
    "    \"pickup_time\",\n",
    "    \"accept_time\",\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"pickup_eta_minutes\",\n",
    "    \"pickup_time_delay\",\n",
    "    \"pickup_distance_km\",\n",
    "    \"cluster\",\n",
    "    \"avg_pickup_time_minutes\",\n",
    "    \"pickup_order_count\",\n",
    "    \"city\",\n",
    "    \"city_order_count\",\n",
    "    \"is_delayed\",\n",
    "    \"speed_kmh\",\n",
    "    \"speed_status\",\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "final_predictions = predictions.select(*selected_columns)\n",
    "\n",
    "# Define PostgreSQL connection details\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "final_predictions.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"feature_engg_pickup_data\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=properties,\n",
    ")\n",
    "print(\"Data Saved to Postgresql successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8f7b237-234e-4d30-99ea-795e457fb649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to csv successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned DataFrame to csv\n",
    "output_path = r'C:\\Users\\Dusty\\Downloads\\Internship\\Last-Mile-Delivery-Delays-and-Route-Optimization\\data\\feature_engg_pickup_data.csv'\n",
    "final_predictions.coalesce(1).write.option(\"header\", \"true\").csv(output_path, mode=\"overwrite\")\n",
    "print(\"Data saved to csv successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
